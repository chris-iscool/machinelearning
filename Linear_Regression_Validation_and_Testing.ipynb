{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression Validation and Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFN457WFDtrojF8WH/A2MQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chris-iscool/machinelearning/blob/main/Linear_Regression_Validation_and_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXeueqiR0Cc_"
      },
      "source": [
        "# Nur für die Tensorflow Version\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtmjm5ie0pqd"
      },
      "source": [
        "# Wichtige imports für Tensorflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diesmal benutzen wir zwei Datensätze, einen Training und einen Test Satz. Im weiteren werden wir auch vom Training Satz noch einen Teil absplitten als Validation Satz."
      ],
      "metadata": {
        "id": "5gZmbFd5WbYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_train_df = pd.read_csv(filepath_or_buffer=\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "housing_test_df = pd.read_csv(filepath_or_buffer=\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")\n",
        "\n",
        "# Der median house value wird wieder skaliert in beiden Datensätzen\n",
        "scaling_factor = 1000.0\n",
        "housing_train_df[\"median_house_value\"] /= scaling_factor\n",
        "housing_test_df[\"median_house_value\"] /= scaling_factor"
      ],
      "metadata": {
        "id": "LA5edUeXoE9p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir übernehmen die build_model und train_model Methoden aus den anderen bisherigen Notebooks. Als Neuerung führen wir den Validation Split ein, womit das Modell mit einem Teil des Training Satzes trainiert und mit dem Rest validiert wird. Dies wird von den Tensorflow Methoden automatisch ausgeführt, der validation split ist also ein einfaches Argument für die Methode."
      ],
      "metadata": {
        "id": "mhaNKbJnXU6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(eta):\n",
        "  # Ein einfaches Model, ein Layer, ein Knoten (Neuron), eine Funktion\n",
        "  f = tf.keras.models.Sequential()\n",
        "\n",
        "  f.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n",
        "\n",
        "  # Alle weiteren Eigenschaften des 1 Neuron Model werden hier bestimmt, wie Lernen durch Root Mean Square\n",
        "  f.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=eta), loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return f\n",
        "\n",
        "# Training des Neurons:\n",
        "def train_model(model, dataframe, feature, label, epochs_T, batchsize=None, my_validation_split=0.1):\n",
        "  # Wir trainieren das Neuron für eine gegebene Anzahl an Epochen\n",
        "  training_history = model.fit(x=dataframe[feature], y=dataframe[label], batch_size=batchsize, epochs=epochs_T, validation_split=my_validation_split)\n",
        "\n",
        "  # Diesmal geben wir die history an sich zurück, nicht das trainierte weight/bias\n",
        "  epochs = training_history.epoch\n",
        "  states = pd.DataFrame(training_history.history)\n",
        "  rms_errors = states[\"root_mean_squared_error\"]\n",
        "\n",
        "  return [epochs, rms_errors, training_history.history]"
      ],
      "metadata": {
        "id": "jjDE1AGgXRU8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model(dataframe, feature, label, trained_weight, trained_bias):\n",
        "  # Labels für die Achsen\n",
        "  plt.figure()\n",
        "  plt.xlabel(feature)\n",
        "  plt.ylabel(label)\n",
        "\n",
        "  # Plotte die \"echten\" Daten\n",
        "  samples = dataframe.sample(n=200)\n",
        "  plt.scatter(samples[feature], samples[label])\n",
        "\n",
        "  # Plotte das Ergebnis der linearen Regression, also eine Gerade mit dem trainierten Gewicht und Bias der letzten Epoche\n",
        "  x0 = 0\n",
        "  y0 = trained_bias\n",
        "  x1 = samples[feature].max()\n",
        "  y1 = y0 + trained_weight * x1\n",
        "\n",
        "  plt.plot([x0, x1], [y0, y1])\n",
        "  plt.show()\n",
        "\n",
        "def plot_loss_curves(epochs, rms_errors_training, rms_errors_validation):\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Root Mean Square Error\")\n",
        "\n",
        "  # Das Element 0 wird nicht mit geplottet, da der Loss bei dem Element unnatürlich hoch ist\n",
        "  plt.plot(epochs[1:], rms_errors_training[1:], label=\"Training loss\")\n",
        "  plt.plot(epochs[1:], rms_errors_validation[1:], label=\"Validation loss\")\n",
        "  plt.legend()\n",
        "\n",
        "  rms_errors_merged = rms_errors_training[1:] + rms_errors_validation[1:]\n",
        "  highest_loss = rms_errors_merged.max()\n",
        "  lowest_loss = rms_errors_merged.min()\n",
        "  delta = highest_loss - lowest_loss\n",
        "  print(delta)\n",
        "\n",
        "  y_axis_top = highest_loss + delta * 0.05\n",
        "  y_axis_bottom = lowest_loss - delta*0.05\n",
        "\n",
        "  plt.ylim([y_axis_bottom, y_axis_top])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "2M27b8Q_oEPe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0DWbjy3zemdj"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}